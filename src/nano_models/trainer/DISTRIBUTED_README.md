# Qwen3åˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—

è¿™ä¸ªæŒ‡å—å°†å¸®åŠ©ä½ ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒæ¥åŠ é€ŸQwen3æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒçš„ä¼˜åŠ¿

### 1. **è®­ç»ƒåŠ é€Ÿ**
- å¤šGPUå¹¶è¡Œè®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´
- æ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
- æ›´å¥½çš„æ¢¯åº¦ä¼°è®¡ï¼Œå¯èƒ½æé«˜æ”¶æ•›é€Ÿåº¦

### 2. **å†…å­˜æ‰©å±•**
- æ¯ä¸ªGPUå¤„ç†éƒ¨åˆ†æ•°æ®ï¼Œå‡å°‘å•GPUå†…å­˜å‹åŠ›
- æ”¯æŒæ›´å¤§çš„æ¨¡å‹å’Œæ›´é•¿çš„åºåˆ—

### 3. **æˆæœ¬æ•ˆç›Š**
- å……åˆ†åˆ©ç”¨å¤šGPUèµ„æº
- å‡å°‘å•æ¬¡è®­ç»ƒçš„æ—¶é—´æˆæœ¬

## ğŸ“ æ–‡ä»¶ç»“æ„

```
trainer/
â”œâ”€â”€ distributed_trainer.py      # åˆ†å¸ƒå¼è®­ç»ƒå™¨æ ¸å¿ƒ
â”œâ”€â”€ run_distributed_training.py # åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ qwen3_trainer.py           # å•GPUè®­ç»ƒå™¨
â””â”€â”€ DISTRIBUTED_README.md      # æœ¬è¯´æ˜æ–‡æ¡£
```

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### 1. **ç¡¬ä»¶è¦æ±‚**
- å¤šGPUç¯å¢ƒï¼ˆæ¨è2-8å¼ GPUï¼‰
- æ¯å¼ GPUè‡³å°‘8GBæ˜¾å­˜ï¼ˆæ¨è16GB+ï¼‰
- GPUä¹‹é—´æ”¯æŒNCCLé€šä¿¡

### 2. **è½¯ä»¶è¦æ±‚**
```bash
# PyTorchç‰ˆæœ¬è¦æ±‚
torch >= 2.0.0
torchvision >= 0.15.0

# ç¡®ä¿æ”¯æŒCUDAå’Œåˆ†å¸ƒå¼è®­ç»ƒ
python -c "import torch; print(torch.cuda.is_available()); print(torch.distributed.is_available())"
```

### 3. **ç½‘ç»œè¦æ±‚**
- å•æœºå¤šå¡ï¼šæ— éœ€é¢å¤–ç½‘ç»œé…ç½®
- å¤šæœºå¤šå¡ï¼šéœ€è¦é…ç½®ç½‘ç»œé€šä¿¡

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1. **å•æœºå¤šå¡è®­ç»ƒ**

```bash
# ä½¿ç”¨2å¼ GPUè®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=2 run_distributed_training.py

# ä½¿ç”¨4å¼ GPUè®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=4 run_distributed_training.py

# ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
python -m torch.distributed.launch --nproc_per_node=8 run_distributed_training.py
```

### 2. **è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ**

```bash
python -m torch.distributed.launch --nproc_per_node=4 run_distributed_training.py \
    --batch_size 4 \
    --max_length 256 \
    --num_epochs 10 \
    --model_size medium \
    --lr 0.0005 \
    --save_dir ./my_checkpoints
```

### 3. **å•GPUè®­ç»ƒï¼ˆå…¼å®¹æ€§ï¼‰**

```bash
# ç›´æ¥è¿è¡Œï¼Œè‡ªåŠ¨æ£€æµ‹ä¸ºå•GPUæ¨¡å¼
python run_distributed_training.py --batch_size 8 --num_epochs 5
```

## ğŸ“Š å‚æ•°è¯´æ˜

### 1. **è®­ç»ƒå‚æ•°**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--local_rank` | int | -1 | æœ¬åœ°GPUæ’åï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰ |
| `--world_size` | int | -1 | æ€»GPUæ•°é‡ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰ |
| `--batch_size` | int | 2 | æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å° |
| `--max_length` | int | 128 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--num_epochs` | int | 3 | è®­ç»ƒè½®æ•° |
| `--model_size` | str | tiny | æ¨¡å‹å¤§å°ï¼ˆtiny/small/medium/largeï¼‰ |
| `--lr` | float | 0.001 | å­¦ä¹ ç‡ |
| `--save_dir` | str | ./qwen3_distributed_checkpoints | ä¿å­˜ç›®å½• |

### 2. **æ¨¡å‹å¤§å°é…ç½®**

| æ¨¡å‹å¤§å° | å‚æ•°æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|----------|
| `tiny` | ~22M | 2-4GB | å¿«é€Ÿæµ‹è¯•ã€åŸå‹å¼€å‘ |
| `small` | ~6M | 4-8GB | å°è§„æ¨¡ä»»åŠ¡ã€èµ„æºå—é™ |
| `medium` | ~24M | 8-16GB | ä¸­ç­‰è§„æ¨¡ä»»åŠ¡ |
| `large` | ~96M | 16GB+ | å¤§è§„æ¨¡ä»»åŠ¡ã€ç”Ÿäº§ç¯å¢ƒ |

## ğŸ”„ åˆ†å¸ƒå¼è®­ç»ƒæµç¨‹

### 1. **ç¯å¢ƒåˆå§‹åŒ–**
```python
# è‡ªåŠ¨æ£€æµ‹åˆ†å¸ƒå¼ç¯å¢ƒ
if args.local_rank != -1:
    setup_distributed_training(args.local_rank, args.world_size)
    is_distributed = True
else:
    is_distributed = False
```

### 2. **æ¨¡å‹åŒ…è£…**
```python
# åˆ†å¸ƒå¼è®­ç»ƒæ—¶è‡ªåŠ¨åŒ…è£…ä¸ºDDP
if self.is_distributed:
    self.model = DDP(self.model, device_ids=[local_rank], output_device=local_rank)
```

### 3. **æ•°æ®åˆ†å‘**
```python
# ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨
if self.is_distributed:
    sampler = DistributedSampler(
        dataset, 
        num_replicas=self.world_size, 
        rank=self.rank,
        shuffle=True
    )
```

### 4. **æ¢¯åº¦åŒæ­¥**
```python
# è‡ªåŠ¨æ¢¯åº¦åŒæ­¥ï¼ˆDDPå¤„ç†ï¼‰
loss.backward()
self.optimizer.step()
```

### 5. **æŸå¤±åŒæ­¥**
```python
# åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­åŒæ­¥æŸå¤±
if self.is_distributed:
    avg_loss_tensor = torch.tensor(avg_loss, device=self.device)
    dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)
    avg_loss = avg_loss_tensor.item() / self.world_size
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. **æ‰¹æ¬¡å¤§å°è°ƒä¼˜**
```bash
# æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å°
# æ˜¾å­˜å……è¶³æ—¶å¢åŠ æ‰¹æ¬¡å¤§å°
--batch_size 8  # æ¯GPU 8ä¸ªæ ·æœ¬

# æ˜¾å­˜ä¸è¶³æ—¶å‡å°‘æ‰¹æ¬¡å¤§å°
--batch_size 1  # æ¯GPU 1ä¸ªæ ·æœ¬
```

### 2. **åºåˆ—é•¿åº¦ä¼˜åŒ–**
```bash
# æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´åºåˆ—é•¿åº¦
--max_length 64   # çŸ­æ–‡æœ¬ä»»åŠ¡
--max_length 512  # é•¿æ–‡æœ¬ä»»åŠ¡
--max_length 1024 # è¶…é•¿æ–‡æœ¬ä»»åŠ¡
```

### 3. **å­¦ä¹ ç‡è°ƒæ•´**
```bash
# åˆ†å¸ƒå¼è®­ç»ƒæ—¶å¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡
--lr 0.001    # é»˜è®¤å­¦ä¹ ç‡
--lr 0.0005   # é™ä½å­¦ä¹ ç‡
--lr 0.002    # æé«˜å­¦ä¹ ç‡
```

### 4. **æ•°æ®åŠ è½½ä¼˜åŒ–**
```python
# å¤šè¿›ç¨‹æ•°æ®åŠ è½½
DataLoader(
    dataset,
    batch_size=batch_size,
    sampler=sampler,
    num_workers=4,      # å¤šè¿›ç¨‹
    pin_memory=True     # å†…å­˜å›ºå®š
)
```

## ğŸš¨ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. **CUDAå†…å­˜ä¸è¶³**
```bash
# è§£å†³æ–¹æ¡ˆ1ï¼šå‡å°‘æ‰¹æ¬¡å¤§å°
--batch_size 1

# è§£å†³æ–¹æ¡ˆ2ï¼šå‡å°‘åºåˆ—é•¿åº¦
--max_length 64

# è§£å†³æ–¹æ¡ˆ3ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹
--model_size tiny
```

### 2. **NCCLé€šä¿¡é”™è¯¯**
```bash
# è§£å†³æ–¹æ¡ˆ1ï¼šè®¾ç½®ç¯å¢ƒå˜é‡
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1

# è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨GLOOåç«¯ï¼ˆCPUè®­ç»ƒï¼‰
# ä¿®æ”¹ distributed_trainer.py ä¸­çš„åç«¯è®¾ç½®
```

### 3. **è¿›ç¨‹åŒæ­¥é—®é¢˜**
```bash
# è§£å†³æ–¹æ¡ˆï¼šç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ—¶å¯åŠ¨
# ä½¿ç”¨ torch.distributed.launch è‡ªåŠ¨å¤„ç†
python -m torch.distributed.launch --nproc_per_node=N script.py
```

### 4. **æ¨¡å‹ä¿å­˜å¤±è´¥**
```python
# è§£å†³æ–¹æ¡ˆï¼šåªåœ¨ä¸»è¿›ç¨‹ä¸­ä¿å­˜
if self.rank == 0:
    self.save("model.pt")
```

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### 1. **è®­ç»ƒç›‘æ§**
```python
# æŸ¥çœ‹è®­ç»ƒç»Ÿè®¡
stats = trainer.get_training_stats()
print(f"è¿›ç¨‹æ’å: {stats['rank']}")
print(f"æ€»è¿›ç¨‹æ•°: {stats['world_size']}")
print(f"æ˜¯å¦åˆ†å¸ƒå¼: {stats['is_distributed']}")
```

### 2. **æ€§èƒ½åˆ†æ**
```bash
# ä½¿ç”¨nvidia-smiç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi

# ä½¿ç”¨torch profileråˆ†ææ€§èƒ½
python -m torch.utils.bottleneck script.py
```

### 3. **æ—¥å¿—åˆ†æ**
```python
# åˆ†å¸ƒå¼è®­ç»ƒæ—¥å¿—ç¤ºä¾‹
# è¿›ç¨‹ 0/4 åœ¨ GPU 0 ä¸Šåˆå§‹åŒ–å®Œæˆ
# è¿›ç¨‹ 1/4 åœ¨ GPU 1 ä¸Šåˆå§‹åŒ–å®Œæˆ
# è¿›ç¨‹ 2/4 åœ¨ GPU 2 ä¸Šåˆå§‹åŒ–å®Œæˆ
# è¿›ç¨‹ 3/4 åœ¨ GPU 3 ä¸Šåˆå§‹åŒ–å®Œæˆ
# åˆ†å¸ƒå¼Qwen3è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ
# æ€»æ‰¹æ¬¡å¤§å°: 8  # 2 * 4
```

## ğŸš€ é«˜çº§ç”¨æ³•

### 1. **å¤šæœºå¤šå¡è®­ç»ƒ**
```bash
# æœºå™¨1ï¼ˆä¸»èŠ‚ç‚¹ï¼‰
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr="192.168.1.100" \
    --master_port=12355 \
    run_distributed_training.py

# æœºå™¨2ï¼ˆä»èŠ‚ç‚¹ï¼‰
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr="192.168.1.100" \
    --master_port=12355 \
    run_distributed_training.py
```

### 2. **æ··åˆç²¾åº¦è®­ç»ƒ**
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    outputs = self.model(input_ids)
    loss = self.criterion(outputs_flat, targets_flat)

scaler.scale(loss).backward()
scaler.step(self.optimizer)
scaler.update()
```

### 3. **æ¢¯åº¦ç´¯ç§¯**
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
accumulation_steps = 4

if (batch_idx + 1) % accumulation_steps == 0:
    self.optimizer.step()
    self.optimizer.zero_grad()
```

## ğŸ“š å®Œæ•´ç¤ºä¾‹

### 1. **å¿«é€Ÿæµ‹è¯•**
```bash
# ä½¿ç”¨2å¼ GPUå¿«é€Ÿæµ‹è¯•
python -m torch.distributed.launch --nproc_per_node=2 run_distributed_training.py \
    --model_size tiny \
    --num_epochs 1 \
    --batch_size 1
```

### 2. **ç”Ÿäº§è®­ç»ƒ**
```bash
# ä½¿ç”¨4å¼ GPUè¿›è¡Œç”Ÿäº§è®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=4 run_distributed_training.py \
    --model_size medium \
    --num_epochs 100 \
    --batch_size 4 \
    --max_length 512 \
    --lr 0.0005 \
    --save_dir ./production_checkpoints
```

### 3. **å¤§è§„æ¨¡è®­ç»ƒ**
```bash
# ä½¿ç”¨8å¼ GPUè¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=8 run_distributed_training.py \
    --model_size large \
    --num_epochs 200 \
    --batch_size 2 \
    --max_length 1024 \
    --lr 0.0001
```

## ğŸ¤ æ•…éšœæ’é™¤

### 1. **æ£€æŸ¥åˆ†å¸ƒå¼ç¯å¢ƒ**
```python
import torch.distributed as dist

print(f"åˆ†å¸ƒå¼æ˜¯å¦åˆå§‹åŒ–: {dist.is_initialized()}")
if dist.is_initialized():
    print(f"å½“å‰æ’å: {dist.get_rank()}")
    print(f"æ€»è¿›ç¨‹æ•°: {dist.get_world_size()}")
```

### 2. **æ£€æŸ¥GPUçŠ¶æ€**
```bash
# æ£€æŸ¥GPUæ•°é‡å’ŒçŠ¶æ€
nvidia-smi

# æ£€æŸ¥CUDAç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"
```

### 3. **æ£€æŸ¥ç½‘ç»œé…ç½®**
```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
netstat -an | grep 12355

# æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
sudo ufw status
```

## ğŸ“– å‚è€ƒèµ„æ–™

- [PyTorchåˆ†å¸ƒå¼è®­ç»ƒå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html)
- [NCCLåç«¯æ–‡æ¡£](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/)
- [åˆ†å¸ƒå¼è®­ç»ƒæœ€ä½³å®è·µ](https://pytorch.org/tutorials/beginner/dist_overview.html)

## ğŸ‰ æ€»ç»“

ä½¿ç”¨è¿™ä¸ªåˆ†å¸ƒå¼è®­ç»ƒå™¨ï¼Œä½ å¯ä»¥ï¼š

1. **è½»æ¾æ‰©å±•åˆ°å¤šGPUç¯å¢ƒ**
2. **æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹**
3. **å¤„ç†æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®**
4. **ä¿æŒä¸å•GPUè®­ç»ƒçš„å…¼å®¹æ€§**

è®°ä½ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ›´å¤šçš„é…ç½®å’Œè°ƒè¯•ï¼Œä½†ä¸€æ—¦é…ç½®æ­£ç¡®ï¼Œè®­ç»ƒæ•ˆç‡å°†å¤§å¹…æå‡ï¼
