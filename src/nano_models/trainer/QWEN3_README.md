# Qwen3æ¨¡å‹è®­ç»ƒå™¨ä½¿ç”¨æŒ‡å—

è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºQwen3Transformeræ¨¡å‹è®¾è®¡çš„è®­ç»ƒå™¨ï¼ŒåŒ…å«å®Œæ•´çš„æ•°æ®å¤„ç†ã€è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½ã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

- **ä¸“ç”¨è®­ç»ƒå™¨**: ä¸“é—¨ä¸ºQwen3Transformeræ¨¡å‹ä¼˜åŒ–
- **æ™ºèƒ½æ•°æ®å¤„ç†**: è‡ªåŠ¨å¤„ç†å˜é•¿åºåˆ—å’Œå¡«å……
- **å†…ç½®åˆ†è¯å™¨**: ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨ï¼Œæ”¯æŒä¸­æ–‡
- **æ–‡æœ¬ç”Ÿæˆ**: è®­ç»ƒåå¯ç›´æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
- **è‡ªåŠ¨ä¿å­˜**: æ”¯æŒæ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¨³å®šè®­ç»ƒ

## ğŸ“ æ–‡ä»¶ç»“æ„

```
trainer/
â”œâ”€â”€ qwen3_trainer.py      # Qwen3ä¸“ç”¨è®­ç»ƒå™¨
â”œâ”€â”€ qwen3_example.py      # å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ trainer.py            # é€šç”¨è®­ç»ƒå™¨
â””â”€â”€ QWEN3_README.md      # æœ¬è¯´æ˜æ–‡æ¡£
```

## ğŸ”§ å®‰è£…ä¾èµ–

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install torch>=2.8.0
```

## ğŸ“– å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from qwen3_trainer import Qwen3Trainer, SimpleTokenizer, TextDataset
from nano_models.model.qwen3 import Qwen3Config, Qwen3Transformer

# åˆ›å»ºæ¨¡å‹é…ç½®
config = Qwen3Config.from_preset('tiny')  # å¯é€‰: tiny, small, medium, large
model = Qwen3Transformer(config)

# åˆ›å»ºåˆ†è¯å™¨å’Œæ•°æ®é›†
tokenizer = SimpleTokenizer(vocab_size=1000)
texts = ["ä½ çš„è®­ç»ƒæ–‡æœ¬1", "ä½ çš„è®­ç»ƒæ–‡æœ¬2", ...]
dataset = TextDataset(texts, tokenizer, max_length=512)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Qwen3Trainer(
    model=model,
    dataset=dataset,
    tokenizer=tokenizer,
    optimizer=optimizer,
    criterion=criterion,
    batch_size=4
)

# å¼€å§‹è®­ç»ƒ
trainer.train(num_epochs=10)
```

### 2. æ¨¡å‹é…ç½®é€‰é¡¹

```python
# ä½¿ç”¨é¢„è®¾é…ç½®
config = Qwen3Config.from_preset('tiny')    # çº¦1.5Må‚æ•°
config = Qwen3Config.from_preset('small')   # çº¦6Må‚æ•°
config = Qwen3Config.from_preset('medium')  # çº¦24Må‚æ•°
config = Qwen3Config.from_preset('large')   # çº¦96Må‚æ•°

# è‡ªå®šä¹‰é…ç½®
config = Qwen3Config(
    hidden_size=512,
    vocab_size=32000,
    intermediate_size=2048,
    num_decoder_layer=12,
    head_dim=64,
    num_attention_heads=8,
    num_key_value_heads=8,
    num_key_value_groups=1,
    attention_bias=False
)
```

### 3. è®­ç»ƒå‚æ•°è°ƒä¼˜

```python
# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = optim.AdamW(
    model.parameters(),
    lr=0.001,           # å­¦ä¹ ç‡
    weight_decay=0.01   # æƒé‡è¡°å‡
)

# åˆ›å»ºæŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

# è®­ç»ƒå™¨é…ç½®
trainer = Qwen3Trainer(
    model=model,
    dataset=dataset,
    tokenizer=tokenizer,
    optimizer=optimizer,
    criterion=criterion,
    batch_size=4,           # æ‰¹æ¬¡å¤§å°
    device="auto",          # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    save_dir="./checkpoints", # ä¿å­˜ç›®å½•
    max_length=512          # æœ€å¤§åºåˆ—é•¿åº¦
)
```

## ğŸ“Š æ•°æ®å¤„ç†

### 1. æ–‡æœ¬æ•°æ®é›†

```python
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # è‡ªåŠ¨æ„å»ºè¯æ±‡è¡¨
        self.tokenizer.build_vocab(texts)
        
        # é¢„å¤„ç†æ–‡æœ¬
        self.processed_data = []
        for text in texts:
            tokens = self.tokenizer.encode(text, max_length)
            if len(tokens) >= 2:
                self.processed_data.append(tokens)
    
    def __getitem__(self, idx):
        tokens = self.processed_data[idx]
        # è¾“å…¥: é™¤äº†æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰token
        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)
        # ç›®æ ‡: é™¤äº†ç¬¬ä¸€ä¸ªtokençš„æ‰€æœ‰token
        target_ids = torch.tensor(tokens[1:], dtype=torch.long)
        return input_ids, target_ids
```

### 2. åˆ†è¯å™¨åŠŸèƒ½

```python
tokenizer = SimpleTokenizer(vocab_size=1000)

# æ„å»ºè¯æ±‡è¡¨
tokenizer.build_vocab(texts, min_freq=2)

# ç¼–ç æ–‡æœ¬
tokens = tokenizer.encode("ä½ å¥½ä¸–ç•Œ", max_length=64)

# è§£ç token
text = tokenizer.decode(tokens)
```

## ğŸ¯ è®­ç»ƒè¿‡ç¨‹

### 1. å¼€å§‹è®­ç»ƒ

```python
# åŸºæœ¬è®­ç»ƒ
trainer.train(num_epochs=10)

# é«˜çº§è®­ç»ƒé…ç½®
trainer.train(
    num_epochs=20,      # è®­ç»ƒè½®æ•°
    save_every=5,       # æ¯5è½®ä¿å­˜ä¸€æ¬¡
    log_every=10        # æ¯10æ­¥è®°å½•ä¸€æ¬¡
)
```

### 2. è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- æ¯ä¸ªepochçš„è¿›åº¦
- æ¯ä¸ªbatchçš„æŸå¤±
- å¹³å‡æŸå¤±
- è‡ªåŠ¨ä¿å­˜ä¿¡æ¯

### 3. è®­ç»ƒç‰¹æ€§

- **æ¢¯åº¦è£å‰ª**: è‡ªåŠ¨é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **è‡ªåŠ¨ä¿å­˜**: å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹å’Œæœ€ä½³æ¨¡å‹
- **æŸå¤±è®¡ç®—**: å¿½ç•¥å¡«å……tokençš„æŸå¤±
- **è®¾å¤‡ç®¡ç†**: è‡ªåŠ¨å¤„ç†CPU/GPUæ•°æ®è½¬ç§»

## ğŸ” æ¨¡å‹è¯„ä¼°

```python
# è¯„ä¼°è®­ç»ƒæ•°æ®é›†
eval_loss = trainer.eval()

# è¯„ä¼°è‡ªå®šä¹‰æ•°æ®é›†
eval_dataset = TextDataset(eval_texts, tokenizer, max_length=512)
eval_loss = trainer.eval(eval_dataset)
```

## âœ¨ æ–‡æœ¬ç”Ÿæˆ

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
# ç”Ÿæˆæ–‡æœ¬
prompt = "äººå·¥æ™ºèƒ½"
generated = trainer.generate_text(
    prompt=prompt,
    max_length=100,    # æœ€å¤§ç”Ÿæˆé•¿åº¦
    temperature=0.8    # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
)

print(f"æç¤º: {prompt}")
print(f"ç”Ÿæˆ: {generated}")
```

## ğŸ’¾ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

### 1. è‡ªåŠ¨ä¿å­˜

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä¿å­˜ï¼š
- `qwen3_checkpoint_epoch_X.pt`: å®šæœŸæ£€æŸ¥ç‚¹
- `qwen3_best_model.pt`: æœ€ä½³æ¨¡å‹

### 2. æ‰‹åŠ¨ä¿å­˜

```python
trainer.save("my_model.pt")
```

### 3. åŠ è½½æ¨¡å‹

```python
# åŠ è½½æ£€æŸ¥ç‚¹
success = trainer.load("qwen3_checkpoints/qwen3_best_model.pt")

if success:
    print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    # ç»§ç»­è®­ç»ƒæˆ–è¿›è¡Œæ¨ç†
```

## ğŸ“ˆ è®­ç»ƒç»Ÿè®¡

```python
# è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
stats = trainer.get_training_stats()

print(f"å½“å‰è½®æ•°: {stats['current_epoch']}")
print(f"æœ€ä½³æŸå¤±: {stats['best_loss']}")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {stats['total_training_steps']}")
print(f"ä½¿ç”¨è®¾å¤‡: {stats['device']}")
print(f"è¯æ±‡è¡¨å¤§å°: {stats['vocab_size']}")
print(f"æœ€å¤§åºåˆ—é•¿åº¦: {stats['max_length']}")
```

## ğŸ¨ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
class CustomLoss(nn.Module):
    def __init__(self, alpha=0.1):
        super().__init__()
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)
    
    def forward(self, outputs, targets):
        ce_loss = self.ce_loss(outputs, targets)
        # æ·»åŠ è‡ªå®šä¹‰æŸå¤±é¡¹
        return ce_loss
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=100)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(num_epochs):
    # è®­ç»ƒä»£ç ...
    scheduler.step()
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
with autocast():
    outputs = model(input_ids)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **å†…å­˜ç®¡ç†**: æ ¹æ®GPUå†…å­˜è°ƒæ•´batch_sizeå’Œmax_length
2. **æ•°æ®è´¨é‡**: ç¡®ä¿è®­ç»ƒæ–‡æœ¬è´¨é‡ï¼Œé¿å…é‡å¤å’Œæ— æ„ä¹‰å†…å®¹
3. **è¯æ±‡è¡¨å¤§å°**: æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´vocab_size
4. **åºåˆ—é•¿åº¦**: æ ¹æ®ä»»åŠ¡éœ€æ±‚è®¾ç½®åˆé€‚çš„max_length
5. **æ¢¯åº¦è£å‰ª**: é»˜è®¤max_norm=1.0ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   - å‡å°batch_size
   - å‡å°max_length
   - ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®

2. **è®­ç»ƒæŸå¤±ä¸ä¸‹é™**
   - æ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®
   - æ£€æŸ¥æ•°æ®è´¨é‡
   - å¢åŠ è®­ç»ƒè½®æ•°

3. **ç”Ÿæˆæ–‡æœ¬è´¨é‡å·®**
   - å¢åŠ è®­ç»ƒæ•°æ®
   - è°ƒæ•´temperatureå‚æ•°
   - æ£€æŸ¥è¯æ±‡è¡¨æ„å»º

### è°ƒè¯•æŠ€å·§

```python
# æ£€æŸ¥æ¨¡å‹å‚æ•°
print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# æ£€æŸ¥æ•°æ®æ ¼å¼
sample = dataset[0]
print(f"è¾“å…¥å½¢çŠ¶: {sample[0].shape}")
print(f"ç›®æ ‡å½¢çŠ¶: {sample[1].shape}")

# æ£€æŸ¥è®¾å¤‡
print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
print(f"æ•°æ®è®¾å¤‡: {sample[0].device}")
```

## ğŸ“š å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `qwen3_example.py` æ–‡ä»¶è·å–å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®å‡†å¤‡
- æ¨¡å‹åˆ›å»º
- è®­ç»ƒè¿‡ç¨‹
- æ–‡æœ¬ç”Ÿæˆ
- æ¨¡å‹åŠ è½½æµ‹è¯•

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªè®­ç»ƒå™¨ï¼

## ï¿½ï¿½ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚
